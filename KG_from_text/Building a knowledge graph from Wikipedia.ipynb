{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e036a61f",
   "metadata": {},
   "source": [
    "[https://neptune.ai/blog/web-scraping-and-knowledge-graphs-machine-learning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e70fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests in /home/ismail/anaconda3/lib/python3.9/site-packages (from wikipedia-api) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ismail/anaconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ismail/anaconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ismail/anaconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ismail/anaconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Installing collected packages: wikipedia-api\n",
      "Successfully installed wikipedia-api-0.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7afaf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d506e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia(name_topic, verbose=True, user_agent='Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'):\n",
    "    def link_to_wikipedia(link):\n",
    "        try:\n",
    "            page = api_wikipedia.page(link)\n",
    "            if page.exists():\n",
    "                return {'page': link, 'text': page.text, 'link': page.fullurl, 'categories': list(page.categories.keys())}\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    api_wikipedia = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI, user_agent=user_agent)\n",
    "    name_of_page = api_wikipedia.page(name_topic)\n",
    "    if not name_of_page.exists():\n",
    "        print('Page {} is not present'.format(name_of_page))\n",
    "        return\n",
    "\n",
    "    links_to_page = list(name_of_page.links.keys())\n",
    "    procceed = tqdm(desc='Scraped links', unit='', total=len(links_to_page)) if verbose else None\n",
    "    origin = [{'page': name_topic, 'text': name_of_page.text, 'link': name_of_page.fullurl, 'categories': list(name_of_page.categories.keys())}]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        links_future = {executor.submit(link_to_wikipedia, link): link for link in links_to_page}\n",
    "        for future in concurrent.futures.as_completed(links_future):\n",
    "            info = future.result()\n",
    "            origin.append(info) if info else None\n",
    "            procceed.update(1) if verbose else None\n",
    "    procceed.close() if verbose else None\n",
    "\n",
    "    namespaces = ('Wikipedia', 'Special', 'Talk', 'LyricWiki', 'File', 'MediaWiki',\n",
    "                 'Template', 'Help', 'User', 'Category talk', 'Portal talk')\n",
    "    origin = pd.DataFrame(origin)\n",
    "    origin = origin[(len(origin['text']) > 20)\n",
    "                     & ~(origin['page'].str.startswith(namespaces, na=True))]\n",
    "    origin['categories'] = origin.categories.apply(lambda a: [b[9:] for b in a])\n",
    "\n",
    "    origin['topic'] = name_topic\n",
    "    print('Scraped pages', len(origin))\n",
    "\n",
    "    return origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac99631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraped links: 100%|██████████████████████████| 2389/2389 [02:39<00:00, 14.95/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped pages 2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_wikipedia = scrape_wikipedia('COVID 19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2399fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wikipedia.to_csv('data/scraped_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2238e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bceda686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e59e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as ntx\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97fad29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(sents):\n",
    "    # chunk one\n",
    "    enti_one = \"\"\n",
    "    enti_two = \"\"\n",
    "\n",
    "    dep_prev_token = \"\" # dependency tag of previous token in sentence\n",
    "\n",
    "    txt_prev_token = \"\" # previous token in sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "\n",
    "\n",
    "    for tokn in nlp(sents):\n",
    "       # chunk two\n",
    "       ## move to next token if token is punctuation\n",
    "\n",
    "        if tokn.dep_ != \"punct\":\n",
    "            #  check if token is compound word or not\n",
    "            if tokn.dep_ == \"compound\":\n",
    "                prefix = tokn.text\n",
    "                # add the current word to it if the previous word is 'compound’\n",
    "                if dep_prev_token == \"compound\":\n",
    "                    prefix = txt_prev_token + \" \"+ tokn.text\n",
    "\n",
    "            # verify if token is modifier or not\n",
    "            if tokn.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tokn.text\n",
    "                # add it to the current word if the previous word is 'compound'\n",
    "                if dep_prev_token == \"compound\":\n",
    "                    modifier = txt_prev_token + \" \"+ tokn.text\n",
    "\n",
    "            # chunk3\n",
    "            if tokn.dep_.find(\"subj\") == True:\n",
    "                enti_one = modifier +\" \"+ prefix + \" \"+ tokn.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                dep_prev_token = \"\"\n",
    "                txt_prev_token = \"\"\n",
    "\n",
    "            # chunk4\n",
    "            if tokn.dep_.find(\"obj\") == True:\n",
    "                enti_two = modifier +\" \"+ prefix +\" \"+ tokn.text\n",
    "\n",
    "            # chunk 5\n",
    "            # update variable\n",
    "            dep_prev_token = tokn.dep_\n",
    "            txt_prev_token = tokn.text\n",
    "\n",
    "    return [enti_one.strip(), enti_two.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67ab206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:15<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "pairs_of_entities = []\n",
    "for i in tqdm(data_wikipedia['text'][:10]):\n",
    "    pairs_of_entities.append(extract_entities(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a427396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2019 Novel Coronavirus', 'COVID syndrome assessment clinics'],\n",
       " ['= = =', '= = Notes'],\n",
       " ['who', 'Kary Cookies'],\n",
       " ['Tablighi thousands', 'Official Nizamuddin Markaz'],\n",
       " ['relevant SARS that', 'time'],\n",
       " ['Bara Kahu', 'pandemic Malaysia Pakistan'],\n",
       " ['stock  market', 'pandemic stock market Russia'],\n",
       " ['chimeric protein candidate', 'clinical  trials'],\n",
       " ['medical  who', '= = ='],\n",
       " ['August Internet method', 'Oral Education education']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_of_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f96fc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_relation(sent):\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    pattern = [{'DEP':'ROOT'},\n",
    "           {'DEP':'prep','OP':\"?\"},\n",
    "           {'DEP':'agent','OP':\"?\"}, \n",
    "           {'POS':'ADJ','OP':\"?\"}]\n",
    "\n",
    "    matcher.add(\"matching_1\", [pattern])\n",
    "\n",
    "    matcher = matcher(doc)\n",
    "    h = len(matcher) - 1\n",
    "\n",
    "    span = doc[matcher[h][1]:matcher[h][2]]\n",
    "\n",
    "    return (span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71d93f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'='"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtain_relation(data_wikipedia['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7069c3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Tablighi Jamaat religious conference that took place at the \"Masjid Jamek Sri Petaling\" in Kuala Lumpur\\'s Sri Petaling district between 27 February to 1 March 2020 became a COVID-19 super-spreader event with more than 3,300 cases being linked to the event. By 19 May 2020, the Malaysian Director-General of Health Noor Hisham Abdullah confirmed that 48% of the country\\'s COVID-19 cases (3,347) had been linked to the Kuala Lumpur Tablighi Jamaat cluster. Additionally, nearly 10% of attendees were overseas visitors, causing COVID-19 to spread to other countries in Southeast Asia. On 8 July 2020, this cluster was declared over by the Ministry of Health.Although much more widespread, the Tabligh event was not the first wave of coronavirus in Malaysia.\\n\\nEvent\\nBetween 27 February and 1 March 2020, the Tablighi Jamaat movement organised an international conference at the \"Masjid Jamek Sri Petaling\" in Sri Petaling, Kuala Lumpur in Malaysia.  The religious gathering was attended by approximately 16,000 attendees (only 12,500 attendees as claimed by the Sri Petaling tabligh group leaders) including about 1,500 from outside Malaysia. Attendees were found to share food, sit close together, and hold hands at the event. According to guests, the leaders of the event did not talk about COVID-19 precautions, but most attendees washed their hands during the event. Malaysian authorities were criticised for allowing the event to go forward.\\n\\nSpread\\nThe Sri Petaling Tablighi Jamaat gathering has been linked to more than 620 COVID-19 cases in March 2020, making it the largest-known centre of transmission of the virus in Southeast Asia at that time. At least seven countries have traced their cases back to the Malaysia event;  most of the 73 COVID-19 cases in Brunei have been linked to the event, as well as 22 in Cambodia, 13 in Indonesia, 10 in Thailand, 5 in Singapore, 2 in the Philippines, and 2 in Vietnam.By 13 March, the Malaysian Ministry of Health had revised the number of Malaysian Tablighi Jamaat participants from 5,000 to 14,500, raising concerns that more positive cases could be discovered. Of the 14,500 participants, 41 tested positive for COVID-19, which brought the total number of cases in Malaysia to 238. By 17 March, the Sri Petaling event had resulted in the biggest increase in COVID-19 cases in Malaysia, with almost two-thirds of the 673 confirmed cases in Malaysia linked to this event.On 29 March, Director-General Noor Hisham Abdullah announced that the Tabligh cluster had reached the fifth generation. By 19 May, Noor Hisham confirmed that 48% of Malaysia\\'s COVID-19 cases (3,347) had been linked to the Sri Petaling tabligh cluster.\\n\\nAftermath\\nThe Sri Petaling Tablighi Jamaat gathering coincided with a domestic political crisis that had been triggered by the resignation of Prime Minister Mahathir Mohamed on 24 February, which led to the collapse of the Pakatan Harapan coalition government and the resignation of Health Minister Dzulkefly Ahmad. Before the formation of a new Perikatan Nasional government under the leadership of Prime Minister Muhyiddin Yassin on 1 March, the Health Ministry would not have a minister for over two weeks until the appointment of Adham Baba on 10 March. During that time, there was only advice from the Health Ministry to minimise public exposure.In response to a rapid rise in cases, Malaysian health authorities on 11 March began tracking around 5,000 Malaysian citizens who were suspected of being exposed to COVID-19 during the Sri Petaling Tablighi Jamaat gathering. Many of the infected had returned to their respective states and communities, which had led to a surge in community transmissions throughout Malaysia.In mid-April, Adham criticized the former PH Government\\'s handling of the Sri Petaling Tabligh Jamaat cluster. In response, Dzulkefy defended the PH government\\'s preparations for COVID-19 and suggested that Prime Minister Muhyiddin, the former Home Affairs Minister, was responsible for the outbreak since public gatherings like the Tabligh Jamaat would have come under his portfolio.By late 2020, a Top Glove factory become the largest cluster of COVID-19 to date after Tablighi Jamaat, reporting more than 7,000 cases in Malaysia.\\n\\nStigmatization\\nThere have been reports of viral infections being blamed on other communities. Indonesian spread has been blamed on weak government regulations on Chinese workers and tourists, and Tablighi Jamaat for the Malaysian outbreak. Chinese community members have also engaged in religious and racial profiling following the spread linked to the Tablighi cluster. Rohingyas have also been targeted and accused of spreading COVID-19.\\n\\nSee also\\n2020 Tablighi Jamaat coronavirus hotspot in Delhi\\n2020 Tablighi Jamaat COVID-19 hotspot in Pakistan\\nTimeline of the COVID-19 pandemic in Malaysia\\n\\n\\n== Notes and references =='"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wikipedia['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20dc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
