{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35967e70-56ec-44f5-8a43-4c08dd11b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastllama import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec34bbf-83c1-4083-bdba-08714b378bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checklist.chk\t     ggml-model-q4_0.bin      tokenizer.model\n",
      "consolidated.00.pth  params.json\n",
      "ggml-model-f16.gguf  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "! ls ../../llama.cpp/models/llama-2-7b-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a78c320-587f-4cee-836c-5f0c55ac89d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m\n",
      "                ___            __    _    _         __ __      \n",
      "                | | '___  ___ _| |_ | |  | |   ___ |  \\  \\ ___ \n",
      "                | |-<_> |<_-<  | |  | |_ | |_ <_> ||     |<_> |\n",
      "                |_| <___|/__/  |_|  |___||___|<___||_|_|_|<___|\n",
      "                                                            \n",
      "                                                                                        \n",
      "                                                                           \n",
      "                                                       .+*+-.                \n",
      "                                                      -%#--                  \n",
      "                                                    :=***%*++=.              \n",
      "                                                   :+=+**####%+              \n",
      "                                                   ++=+*%#                   \n",
      "                                                  .*+++==-                   \n",
      "                  ::--:.                           .**++=::                   \n",
      "                 #%##*++=......                    =*+==-::                   \n",
      "                .@@@*@%*==-==-==---:::::------::==*+==--::                   \n",
      "                 %@@@@+--====+===---=---==+=======+++----:                   \n",
      "                 .%@@*++*##***+===-=====++++++*++*+====++.                   \n",
      "                 :@@%*##%@@%#*%#+==++++++=++***==-=+==+=-                    \n",
      "                  %@%%%%%@%#+=*%*##%%%@###**++++==--==++                     \n",
      "                  #@%%@%@@##**%@@@%#%%%%**++*++=====-=*-                     \n",
      "                  -@@@@@@@%*#%@@@@@@@%%%%#+*%#++++++=*+.                     \n",
      "                   +@@@@@%%*-#@@@@@@@@@@@%%@%**#*#+=-.                       \n",
      "                    #%%###%:  ..+#%@@@@%%@@@@%#+-                            \n",
      "                    :***#*-         ...  *@@@%*+:                            \n",
      "                     =***=               -@%##**.                            \n",
      "                    :#*++                -@#-:*=.                            \n",
      "                     =##-                .%*..##                             \n",
      "                      +*-                 *:  +-                             \n",
      "                      :+-                :+   =.                             \n",
      "                       =-.               *+   =-                             \n",
      "                        :-:-              =--  :::                           \n",
      "                                                                           \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[32;1m[Info]:\u001b[0m \u001b[32mFunc('Model') loading model from ../../llama.cpp/models/llama-2-7b-chat/ggml-model-q4_0.bin - please wait ...\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m[Error]:\u001b[0m \u001b[31mFunc('read_magic_number') invalid model file ../../llama.cpp/models/llama-2-7b-chat/ggml-model-q4_0.bin (bad magic)\n",
      "\u001b[0m\u001b[31;1m[Error]:\u001b[0m \u001b[31mFunc('FastLlama::Params::build') Unable to load model\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to load model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../llama.cpp/models/llama-2-7b-chat/ggml-model-q4_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m      4\u001b[0m         path\u001b[38;5;241m=\u001b[39mMODEL_PATH, \u001b[38;5;66;03m#path to model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;66;03m#number of threads to use\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, \u001b[38;5;66;03m#context size of model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         last_n_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \u001b[38;5;66;03m#size of last n tokens (used for repetition penalty) (Optional)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m#seed for random number generator (Optional)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         n_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, \u001b[38;5;66;03m#batch size (Optional)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         use_mmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m#use mmap to load model (Optional)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llms/lib/python3.11/site-packages/fastllama/api.py:276\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, path, num_threads, n_ctx, last_n_size, seed, tokens_to_keep, n_batch, use_mmap, use_mlock, should_get_all_logits, embedding_eval_enabled, allocate_extra_mem, logger, load_parallel, n_load_parallel_blocks, library_path)\u001b[0m\n\u001b[1;32m    274\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(load_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mbytes\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to load model"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../../llama.cpp/models/llama-2-7b-chat/ggml-model-q4_0.bin\"\n",
    "\n",
    "model = Model(\n",
    "        path=MODEL_PATH, #path to model\n",
    "        num_threads=8, #number of threads to use\n",
    "        n_ctx=512, #context size of model\n",
    "        last_n_size=64, #size of last n tokens (used for repetition penalty) (Optional)\n",
    "        seed=0, #seed for random number generator (Optional)\n",
    "        n_batch=128, #batch size (Optional)\n",
    "        use_mmap=False, #use mmap to load model (Optional)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1d68d-e621-445e-9758-d0b2ff31e0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
