{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7024713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\harri\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\harri\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\harri\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from accelerate) (2.1.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\harri\\anaconda3\\lib\\site-packages (from accelerate) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from accelerate) (0.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\harri\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6db847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 10 01:10:02 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  | 00000000:24:00.0  On |                  N/A |\n",
      "|  0%   34C    P8               9W / 200W |    912MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2220    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3240    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      3244    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      6396    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7496    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7648    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A      8076    C+G   ...Godot\\Godot_v4.1.1-stable_win64.exe    N/A      |\n",
      "|    0   N/A  N/A     10288    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     13928    C+G   ...on\\119.0.2151.72\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     14728    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     15460    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     16148    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     17928    C+G   ...pdnekdrzrea0\\XboxGameBarSpotify.exe    N/A      |\n",
      "|    0   N/A  N/A     19364    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     22476    C+G   ...11.0_x64__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     23540    C+G   ...\\AMD\\CNext\\CNext\\RadeonSoftware.exe    N/A      |\n",
      "|    0   N/A  N/A     24148    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     26460    C+G   ...aming\\Telegram Desktop\\Telegram.exe    N/A      |\n",
      "|    0   N/A  N/A     27228    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     30956    C+G   ...4.0_x64__cv1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     33800    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     35376    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8360e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11347951",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b377ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pickle.load(open('french_lyrics_only_v2.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d9150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_lyrics = ['\\n'.join(lines) for artist, songs in lyrics.items() for title, lines in songs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09f9ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et nous, plébéiens d'à-présent\n",
      "Anciens Egyptiens, nouveaux roturiers bohémiens\n",
      "En aspirant à la justice, nous n’aspirâmes qu'au statut d'humain\n",
      "\n",
      "Et nous demain, vilains d’aujourd'hui, manants d'avant-hier\n",
      "Fils honni par ses propres pères\n",
      "En aspirant à l'égalité, nous n'aspirâmes qu'au statut d'humain\n",
      "\n",
      "Et nous, prolétaires en bas de l'échelle alimentaire\n",
      "Animal servile se rêvant prince de la ville\n",
      "En aspirant à la fraternité, nous n’aspirâmes qu’au statut d'humain\n",
      "C’est comme si ma peau finissait jaunie à cause de mon gilet noir\n",
      "Le malheur ne discrimine point, te voilà maintenant parlant de moi\n",
      "Ô ciel, quelle méprisable condition qu'est devenue la mienne\n",
      "Je ne peux plus dire à qui que ce soit que j'aime\n",
      "C'est comme si ma peau finissait jaunie à cause de mon gilet noir\n",
      "Le malheur ne discrimine point, te voilà maintenant parlant de moi\n",
      "C’est la vie, c'est la France, c'est le monde\n",
      "C'est la fronde, la galère qui nous poussent à l'extrême\n",
      "C'est l'enfance qu'on regrette, plus personne dit je t'aime\n",
      "C'est la vie, c'est la France, c'est le monde\n",
      "C'est la fronde, la galère qui nous poussent à l'extrême\n",
      "C'est l'enfance qu'on regrette, plus personne dit je t'aime\n",
      "\n",
      "Et nous, migrants sans papiers\n",
      "Ancien Congo, apatrides à nouveau\n",
      "En aspirant à la justice, nous n'aspirâmes qu'au statut d'humain\n",
      "\n",
      "Et nous, Européens demain, Africains aujourd'hui\n",
      "Citoyens illicites, couleur de peau anthracite\n",
      "En aspirant à l'égalité, nous n'aspirâmes qu'au statut d'humain\n",
      "\n",
      "C'est comme si ma peau finissait jaunie à cause de mon gilet noir\n",
      "Le malheur ne discrimine point, te voilà maintenant parlant de moi\n",
      "Ô ciel, quelle méprisable condition qu'est devenue la mienne\n",
      "Je ne peux plus dire à qui que ce soit que j'aime\n",
      "C'est comme si ma peau finissait jaunie à cause de mon gilet noir\n",
      "Le malheur ne discrimine point, te voilà maintenant parlant de moi\n",
      "C'est la vie, c'est la France, c'est le monde\n",
      "C'est la fronde, la galère qui nous poussent à l'extrême\n",
      "C'est l'enfance qu'on regrette, plus personne dit je t'aime\n",
      "C'est la vie, c'est la France, c'est le monde\n",
      "C'est la fronde, la galère qui nous poussent à l'extrême\n",
      "C'est l'enfance qu'on regrette, plus personne dit je t'aime\n",
      "\n",
      "Et nous, plébéiens d'à-présent\n",
      "Anciens Egyptiens, nouveaux roturiers bohémiens\n",
      "En aspirant à la justice, nous n'aspirâmes qu'au statut d'humain\n",
      "\n",
      "Et nous demain, vilains d'aujourd'hui, manants d'avant-hier\n",
      "Fils honni par ses propres pères\n",
      "En aspirant à l'égalité, nous n'aspirâmes qu'au statut d'humain\n",
      "\n",
      "C'est comme si ma peau finissait jaunie à cause de mon gilet noir\n",
      "Le malheur ne discrimine point, te voilà maintenant parlant de moi\n",
      "Ô ciel, quelle méprisable condition qu'est devenue la mienne\n",
      "Je ne peux plus dire à qui que ce soit que j'aime\n",
      "C'est comme si ma peau finissait jaunie à cause de mon gilet noir\n",
      "Le malheur ne discrimine point, te voilà maintenant parlant de moi\n",
      "\n",
      "[Bénédiction de Charles Baudelaire]\n",
      "Elle ravale ainsi l'écume de sa haine\n",
      "Et, ne comprenant pas les desseins éternels\n",
      "Elle-même prépare au fond de la Géhenne\n",
      "Les bûchers consacrés aux crimes maternels\n",
      "\n",
      "Pourtant, sous la tutelle invisible d'un Ange\n",
      "L'Enfant déshérité s'enivre de soleil\n",
      "Et dans tout ce qu'il boit et dans tout ce qu'il mange\n",
      "Retrouve l'ambroisie et le nectar vermeil\n"
     ]
    }
   ],
   "source": [
    "print(song_lyrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3838e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(song_lyrics,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af0bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_files(data_text, dest_path):\n",
    "    f = open(dest_path, 'w', encoding=\"utf-8\")\n",
    "    data = ''\n",
    "    for text in data_text:\n",
    "        data += text + ' '\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e51859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 13485\n",
      "Test dataset length: 2380\n"
     ]
    }
   ],
   "source": [
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')\n",
    "\n",
    "print(\"Train dataset length: \"+str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aeb91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asi/gpt-fr-cased-small\")\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "352baf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\anaconda3\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee0330",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04eaed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1468: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"asi/gpt-fr-cased-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f696a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b07d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt-fr-rap\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b564c279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\harri/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\D\\git\\data_science\\machine_learning\\NLP\\Lyricist\\wandb\\run-20231210_011500-9l8zb24e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harrrando/huggingface/runs/9l8zb24e' target=\"_blank\">worldly-armadillo-1</a></strong> to <a href='https://wandb.ai/harrrando/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harrrando/huggingface' target=\"_blank\">https://wandb.ai/harrrando/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harrrando/huggingface/runs/9l8zb24e' target=\"_blank\">https://wandb.ai/harrrando/huggingface/runs/9l8zb24e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11162' max='11162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11162/11162 32:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.924600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.905800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.865600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.875900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.872900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11162, training_loss=3.0422457907067857, metrics={'train_runtime': 2045.9963, 'train_samples_per_second': 87.286, 'train_steps_per_second': 5.456, 'total_flos': 1.1665774706688e+16, 'train_loss': 3.0422457907067857, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "892b11a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11837747",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e56811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "rapper = pipeline('text-generation', model='./gpt-fr-rap', tokenizer='asi/gpt-fr-cased-small', )\n",
    "                #config={'max_length':200, 'output_scores':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4074b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 'Chez moi'\n",
    "res = rapper(start, max_new_tokens=100, do_sample=True, temperature=0.9, top_k=10, top_p=0.92, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38a1e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chez moi, y a pas de place pour la haine, j'ai la haine, j'ai la haine\n",
      "Y a la vie, le monde, le monde, le monde, le monde, le monde, le monde\n",
      "[Kaaris]\n",
      "La vie c'est une course poursuite, la vie c'est une poursuite\n",
      "La vie c'est une course poursuite, la vie c'est une poursuite\n",
      "La vie c'est une course poursuite, la vie c'est une\n"
     ]
    }
   ],
   "source": [
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "833b6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chez moi, t'es qu'une sale pute, tu m'as donne des ailes\n",
      "Tu sais qu'on a tous les memes problemes, on a tous les memes problemes\n",
      "T'inquiete, j'fais tout pour la famille, tu sais qu'on a tous les memes problemes\n",
      "T'inquiete, j'fais tout pour la famille, tu sais qu'on a tous les memes problemes\n",
      "On a tous les memes problemes\n"
     ]
    }
   ],
   "source": [
    "print(res[1]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7bf10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chez moi y'a pas de place pour l'erreur\n",
      "T'as vu la vie en rose, c'est pas un film\n",
      "Mais tu m'as vu en rose, tu m'as vu en rose\n",
      "T'as vu la vie en rose, c'est pas un film\n",
      "Mais tu m'as vu en rose, tu m'as vu en rose\n",
      "Tu m'as vu en rose, tu m'as vu en rose\n",
      "T'as vu la vie en\n"
     ]
    }
   ],
   "source": [
    "print(res[2]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b006022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"De mon retour des pays des merveilles, \"\n",
    "res = rapper(start, max_new_tokens=100, do_sample=True, temperature=0.9, top_k=10, top_p=0.92, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e82e77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De mon retour des pays des merveilles, \n",
      "j'suis un enfant du peuple, j'ai appris a faire le sourd\n",
      "j'suis un enfant du peuple, j'ai appris a faire le sourd\n",
      "j'suis un enfant du peuple, j'ai appris a faire le sourd\n",
      "j'suis un enfant du peuple, j'ai appris a faire le sourd\n",
      "j'suis un enfant du peuple, j'ai appris a faire le sourd\n",
      "j'suis un enfant du peuple, j'ai appris a faire\n"
     ]
    }
   ],
   "source": [
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a095c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De mon retour des pays des merveilles, \n",
      "Je suis un jeune, j'ai des choses a raconter, \n",
      "J'ai pas envie de faire du rap, je veux juste faire du rap \n",
      "Et je suis un jeune, j'ai des choses a raconter, \n",
      "J'ai pas envie de faire du rap, je veux juste faire du rap \n",
      "Et je suis un jeune, j'ai des choses a raconter, \n",
      "Je suis un jeune, j'ai des choses a raconter, \n",
      "Et\n"
     ]
    }
   ],
   "source": [
    "print(res[1]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f6eb7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De mon retour des pays des merveilles, \n",
      "Je suis un jeune d'ici, et moi, je ne vois que toi \n",
      "Tu as le sourire des enfants mais t'es trop belle \n",
      "T'es trop mignonne, t'es trop jolie \n",
      "Je ne t'ai pas dit que je te connaissais \n",
      "Tu ne veux pas me connaitre \n",
      "T'es trop jolie, t'es trop jolie \n",
      "T'es trop mignonne, t'es trop jolie \n",
      "Je t'ai dit que je\n"
     ]
    }
   ],
   "source": [
    "print(res[2]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aeba5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"Baby veut du salot\"\n",
    "res = rapper(start, max_new_tokens=100, do_sample=True, temperature=0.9, top_k=10, top_p=0.92, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99e950c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby veut du salot mais elle est pas prete\n",
      "Mais elle est pas prete\n",
      "Et je sais qu'elle est pas prete\n",
      "Elle veut du salo, j'la fais monter sur l'terrain\n",
      "J'la fais monter sur l'terrain, j'la fais monter sur l'terrain\n",
      "J'la fais monter sur l'terrain, j'la fais monter sur l'terrain\n",
      "J'la fais monter sur l'terrain, j'la\n"
     ]
    }
   ],
   "source": [
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5eaa6047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby veut du salot, j'lui dis : \"T'es pas comme nous\"\n",
      "J'la prends dans l'froc et j'me casse, elle m'en veut, j'lui dis : \"T'es pas comme nous\"\n",
      "Elle m'dit : \"T'es pas comme nous\"\n",
      "Et j'suis dans l'froc, j'lui dis : \"T'es pas comme nous\"\n",
      "J'lui dis : \"T'es pas comme nous\n"
     ]
    }
   ],
   "source": [
    "print(res[1]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6aa61dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby veut du salot mais j'me sens mal a la tete\n",
      "C'est pour mes freres qui m'ont donne la vie, c'est pour mes freres qui m'ont donne la vie\n",
      "Et si j'te dis c'est que c'est vrai, c'est vrai\n",
      "C'est vrai, c'est vrai\n",
      "C'est vrai, c'est vrai\n",
      "C'est vrai, c'est vrai\n",
      "C'est vrai, c'est vrai\n"
     ]
    }
   ],
   "source": [
    "print(res[2]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230d8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
